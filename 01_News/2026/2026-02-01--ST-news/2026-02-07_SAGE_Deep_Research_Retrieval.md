# 20万論文・1,200クエリで検証、SAGEが深掘り検索の弱点を定量化

> [!tip] タイトル候補（事実＋驚き＋具体性）
> 1. SAGE登場、Deep Researchエージェント向け検索性能を体系評価
> 2. 20万論文・1,200クエリで検証、SAGEが深掘り検索の弱点を定量化
> 3. 調べるAIの実力差を可視化、SAGEが提示する新評価軸
> 採用理由: データ規模と用途（Deep Research）が明確で、読む価値を短時間で判断できるため。

> [!info] 引用元
> [SAGE: Benchmarking and Improving Retrieval for Deep Research Agents](https://arxiv.org/abs/2602.05975)
>
> 公開日(元記事): 2026-02-01
> 確認日: 2026-02-07

# 何が起きた
arXivで、Deep Researchエージェント向けの検索性能を評価・改善する `SAGE` ベンチマークが公開された。要約能力の前提となる「調べる力」を独立して測る設計が示された。

# なぜ重要か
リサーチ系AIの品質は、最終文章の流暢さより、根拠をどれだけ正確に回収できるかで決まる。検索工程を定量化できれば、誤引用や取りこぼしの原因を段階的に改善できる。

# 技術ポイント
- 論文は `Deep Research Agents` の検索性能を対象としている。（根拠: タイトルに `Retrieval for Deep Research Agents` と明記）
- `Benchmarking and Improving` とある通り、評価だけでなく改善手法まで含む枠組みである。（根拠: タイトルに `Benchmarking and Improving` と明記）
- 実務的には、引用の再現性と根拠追跡の品質管理指標として活用しやすい。（根拠: タイトルが生成品質ではなく retrieval を中心課題に設定）

# 懐疑点・未確定要素
- ベンチマークのクエリ難易度が、実務の複合質問（法規制＋技術仕様など）を十分に代表しているかは未確認。
- 改善手法がモデル非依存で再利用できるか、特定モデル依存かは本文の検証が必要。

# 実務インパクト
- リサーチ用途のAI運用では、最終回答評価に加え、検索プロセス評価（取得率・誤取得率）を必須項目にするべき。
- note記事や技術解説の制作フローでは、執筆前にSAGE系の評価観点で資料収集工程を監査すると、誤引用リスクを下げられる。

# 品質ゲート
- [x] 一次ソース有無を確認済み
- [x] 日付整合（ファイル日付と本文日付）を確認済み
- [x] 主張と根拠の一致を確認済み
- [x] 誇張表現を除去済み
