# Kimi K2.5: 視覚とテキストを融合したオープンソースのエージェントモデル

## 参照元
- [arXiv: Kimi K2.5: Visual Agentic Intelligence](https://arxiv.org/abs/2602.02276)

## 3行まとめ
- **Kimi Teamが「Kimi K2.5」を発表。テキストと視覚情報の合同事前学習により、高度なマルチモーダルエージェント能力を実現。**
- **「Agent Swarm」フレームワークを導入し、複雑なタスクを並列エージェント群に分解して実行することで、単一エージェント比で最大4.5倍の高速化を達成。**
- **コーディング、視覚推論、エージェントタスクなど多岐にわたるベンチマークでSOTAを記録し、モデルのチェックポイントも公開された。**

## 詳細
Kimi K2.5は、汎用的なエージェント知能（General Agentic Intelligence）を目指して設計された最新のオープンソースモデルです。従来、テキストと視覚は別々に扱われがちでしたが、K2.5ではこれらを「合同最適化」することで、相互に能力を高め合うアーキテクチャを採用しています。

### 技術的ハイライト
- **Joint Text-Vision Pre-training**: テキストと画像のデータを同時に事前学習し、モダリティ間の壁を取り払いました。
- **Zero-vision SFT**: 視覚データを使わないSupervised Fine-Tuningでも、高い視覚理解能力を維持・向上させる手法を導入。
- **Agent Swarm**: 複雑な問題を異質なサブタスクに動的に分解し、それぞれの得意分野を持つエージェントが並列して解決にあたるオーケストレーションフレームワーク。これによりレイテンシが大幅に削減されました。

## 所感
「Agent Swarm」という名前が示す通り、単体の巨大モデルではなく、複数の専門エージェントが群れ（Swarm）として協調動作する方向性が明確になってきました。視覚と言語を低レイヤーで融合させている点も、GUI操作を伴うエージェントタスクにおいては極めて重要です。オープンソースとして公開されたことで、今後このモデルをベースにした自律型エージェントの開発が加速しそうです。
