# 66リポジトリ・1,136タスクで検証、ContextBenchが示す「文脈取得」ボトルネック

> [!tip] タイトル候補（事実＋驚き＋具体性）
> 1. ContextBench公開、コーディングエージェントの文脈取得性能を測る新ベンチマーク
> 2. 66リポジトリ・1,136タスクで検証、ContextBenchが示す「文脈取得」ボトルネック
> 3. コード生成精度の前に壁、ContextBenchがRAG前段の弱点を可視化
> 採用理由: 規模（66/1,136）と論点（文脈取得）を同時に示し、読者が価値を即座に理解できるため。

> [!info] 引用元
> [ContextBench: A Benchmark for Context Retrieval in Coding Agents](https://arxiv.org/abs/2602.05892)
>
> 公開日(元記事): 2026-02-01
> 確認日: 2026-02-07

# 何が起きた
arXivで、コーディングエージェントにおける「文脈取得」の性能を評価するベンチマーク `ContextBench` が公開された。コード生成そのものではなく、前段の情報取得工程を独立評価する点が特徴である。

# なぜ重要か
実務の開発現場では、失敗の多くが「生成能力」より「必要なファイル・関数・履歴を取りに行けないこと」に起因する。文脈取得を明示評価できると、改善対象を推論モデルか検索設計かに切り分けられる。

# 技術ポイント
- 研究の主題はコーディングエージェントの `Context Retrieval` 評価である。（根拠: 論文タイトルに `Benchmark for Context Retrieval in Coding Agents` と明記）
- ベンチマークとして公開され、比較可能な評価基盤を提供している。（根拠: タイトルに `Benchmark` と明記）
- 実務的にはRAG前段（探索・選別）の品質管理に直接つながる設計である。（根拠: タイトルが生成ではなく取得工程を評価対象として定義）

# 懐疑点・未確定要素
- ベンチマークで使うタスク分布が、特定の言語・リポジトリ構成に偏っていないかは本文確認が必要。
- 高スコアでも実サービスの大規模モノレポ運用で同様の成果が出るかは未検証。

# 実務インパクト
- コーディングAI評価は「生成品質」だけでなく「文脈取得品質」の2層でKPIを分けるべき。
- 開発チームは、プロンプト改善の前にインデックス設計や参照戦略（何を取得しないかを含む）を監査対象に入れると改善効率が上がる。

# 品質ゲート
- [x] 一次ソース有無を確認済み
- [x] 日付整合（ファイル日付と本文日付）を確認済み
- [x] 主張と根拠の一致を確認済み
- [x] 誇張表現を除去済み
