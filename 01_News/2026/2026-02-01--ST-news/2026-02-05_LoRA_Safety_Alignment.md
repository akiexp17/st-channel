# 推論LLMの安全性確保には「LoRA」だけで十分？ "Safety Tax"を回避する新手法

> [!info] 引用元
> [LoRA is All You Need for Safety Alignment of Reasoning LLMs](https://arxiv.org/abs/2507.17075)

# 概要
推論能力を持つLLM（Reasoning LLMs）の安全性調整において、拒否データセットを用いたLoRA（Low-Rank Adaptation）適用が、推論能力を損なわずに安全性を確保できることを示した研究。

# 背景
通常、モデルに安全性を学習させる（Safety Alignment）と、元の推論能力が低下する「Safety Tax（安全税）」と呼ばれるトレードオフが発生する。これを回避しつつ安全性を高めることは、高性能なモデル開発における主要な課題だった。

# 詳細
- **手法**: 拒否データセット（有害なプロンプトへの拒否応答）を用いて、SFT（教師あり微調整）段階でLoRAを適用するだけというシンプルな方法。
- **結果**: フルモデルの調整と同等の安全性を達成しつつ、推論ベンチマーク（数学、科学、コード生成など）での性能低下をほぼゼロに抑えた。
- **分析**:
    - ランク1（Rank-1）の更新で十分な効果が得られる。
    - MLPのアッププロジェクション層のみ、あるいは中間層のみの更新が効果的。
    - 理論的には、微調整タスク（安全性）が低ランクであり、ベース能力（推論）が高ランクである場合にLoRAが最も効果的であることが示された。

# 影響・考察
「安全性か性能か」というジレンマに対し、パラメータ効率の良いLoRAが有効であることを示した点は実用的意義が大きい。特に、計算リソースを抑えつつ安全なモデルをデプロイしたい開発者にとって、即座に適用可能な知見である。
