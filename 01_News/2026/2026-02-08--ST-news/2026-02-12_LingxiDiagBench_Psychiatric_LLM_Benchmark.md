# メンタルヘルス診断LLMを多エージェントで検証、LingxiDiagBenchが中国語評価を標準化

> [!tip] タイトル候補（事実＋驚き＋具体性）
> 1. メンタルヘルス診断LLMを多エージェントで検証、LingxiDiagBenchが中国語評価を標準化
> 2. 中国語メンタルヘルス相談を模擬、LingxiDiagBenchが診断LLMの弱点を可視化
> 3. 医療対話AIの見落としを点検、LingxiDiagBenchが多段評価フレームを提示
> 採用理由: 対象領域（メンタルヘルス診療）と新規性（多エージェント評価）が1行で伝わるため。

> [!info] 引用元
> [LingxiDiagBench: A Multi-Agent Framework for Benchmarking LLMs in Chinese Psychiatric Consultation and Diagnosis](https://arxiv.org/abs/2602.09379)
>
> 公開日(元記事): 2026-02-10
> 確認日: 2026-02-12

# 30秒サマリー
メンタルヘルス相談の対話を中国語で再現し、複数エージェントで診断過程を評価するベンチマークが公開された。正答率だけでなく、推論過程の整合性や危険な見落としを点検できる設計で、医療対話LLMの安全評価を一段具体化した。

# 何が起きた
LingxiDiagBenchがarXivに公開された。メンタルヘルスの相談から鑑別、診断判断までを段階化し、単発QAでは測れない臨床推論の連続性を評価対象にした。

# なぜ重要か
医療対話AIは出力が自然でも、診断根拠が不十分だと実運用で危険になる。多エージェント評価により、判断の一貫性とリスクを同時に監査しやすくなる。

# 技術ポイント
- メンタルヘルス相談・診断に特化した中国語評価タスクを定義した（根拠: 論文タイトルの`Chinese Psychiatric Consultation and Diagnosis`）
- 単一回答ではなく多エージェント構成で診断過程を評価する（根拠: タイトルの`Multi-Agent Framework`）
- ベンチマーク目的を明示し比較実験を想定した設計である（根拠: タイトルの`Benchmarking LLMs`）

# 懐疑点・未確定要素
- 言語・文化差が大きい他地域で同等に機能するかは未検証。
- 臨床現場データとの整合は追加検証が必要。

# 実務インパクト
- 医療系LLM導入前に、対話品質だけでなく診断推論の監査項目を追加できる。
- モデル更新時の安全再評価プロトコルを作りやすい。

# 品質ゲート
- [x] 一次ソース有無を確認済み
- [x] 日付整合（ファイル日付と本文日付）を確認済み
- [x] 主張と根拠の一致を確認済み
- [x] 誇張表現を除去済み
- [x] 30〜60秒で読める長さ（250〜450文字）になっている
- [x] 専門用語の初出に補足を付けた
