# 2026-02-11 Ranked News Candidates

- Source inbox: `2026-02-11_RSS_Links.md`
- Total parsed items: 1677
- Deduplicated items: 1677
- Selected items (top 40, min score 5.2): 40

## Scoring rubric
- Total = 0.35*技術新規性 + 0.30*実務影響 + 0.20*信頼性 + 0.15*鮮度
- Each axis is scored on a 0-10 scale

## Top Candidates

### 1. LingxiDiagBench: A Multi-Agent Framework for Benchmarking LLMs in Chinese Psychiatric Consultation and Diagnosis
- URL: https://arxiv.org/abs/2602.09379
- Source: cs.CL updates on arXiv.org
- Score: 6.34 (新規性 5.4 / 実務影響 7.4 / 信頼性 7.0 / 鮮度 5.5)

### 2. MacrOData: New Benchmarks of Thousands of Datasets for Tabular Outlier Detection
- URL: https://arxiv.org/abs/2602.09329
- Source: cs.LG updates on arXiv.org
- Score: 6.28 (新規性 6.6 / 実務影響 5.8 / 信頼性 7.0 / 鮮度 5.5)

### 3. How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study
- URL: https://arxiv.org/abs/2602.07814
- Source: cs.AI updates on arXiv.org
- Score: 6.02 (新規性 6.9 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 4. MAPS: A Multilingual Benchmark for Agent Performance and Security
- URL: https://arxiv.org/abs/2505.15935
- Source: cs.CL updates on arXiv.org
- Score: 5.96 (新規性 5.4 / 実務影響 7.4 / 信頼性 7.0 / 鮮度 3.0)

### 5. Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots
- URL: https://arxiv.org/abs/2602.07434
- Source: cs.AI updates on arXiv.org
- Score: 5.96 (新規性 3.8 / 実務影響 8.0 / 信頼性 7.0 / 鮮度 5.5)

### 6. UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking
- URL: https://arxiv.org/abs/2602.10093
- Source: cs.RO updates on arXiv.org
- Score: 5.92 (新規性 6.6 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 7. ContextBench: A Benchmark for Context Retrieval in Coding Agents
- URL: https://arxiv.org/abs/2602.05892
- Source: cs.LG updates on arXiv.org
- Score: 5.92 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 5.5)

### 8. SWE-AGI: Benchmarking Specification-Driven Software Construction with MoonBit in the Era of Autonomous Agents
- URL: https://arxiv.org/abs/2602.09447
- Source: cs.CL updates on arXiv.org
- Score: 5.92 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 5.5)

### 9. LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth
- URL: https://arxiv.org/abs/2602.07962
- Source: cs.AI updates on arXiv.org
- Score: 5.92 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 5.5)

### 10. From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent
- URL: https://arxiv.org/abs/2602.08412
- Source: cs.AI updates on arXiv.org
- Score: 5.92 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 5.5)

### 11. CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment
- URL: https://arxiv.org/abs/2602.08023
- Source: cs.AI updates on arXiv.org
- Score: 5.92 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 5.5)

### 12. OmniReview: A Large-scale Benchmark and LLM-enhanced Framework for Realistic Reviewer Recommendation
- URL: https://arxiv.org/abs/2602.08896
- Source: cs.AI updates on arXiv.org
- Score: 5.92 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 5.5)

### 13. ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development
- URL: https://arxiv.org/abs/2602.01655
- Source: cs.AI updates on arXiv.org
- Score: 5.92 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 5.5)

### 14. Reasoning With a Star: A Heliophysics Dataset and Benchmark for Agentic Scientific Reasoning
- URL: https://arxiv.org/abs/2511.20694
- Source: cs.AI updates on arXiv.org
- Score: 5.90 (新規性 5.4 / 実務影響 7.2 / 信頼性 7.0 / 鮮度 3.0)

### 15. PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging
- URL: https://arxiv.org/abs/2602.07044
- Source: cs.AI updates on arXiv.org
- Score: 5.86 (新規性 5.4 / 実務影響 5.8 / 信頼性 7.0 / 鮮度 5.5)

### 16. NAAMSE: Framework for Evolutionary Security Evaluation of Agents
- URL: https://arxiv.org/abs/2602.07391
- Source: cs.AI updates on arXiv.org
- Score: 5.83 (新規性 3.8 / 実務影響 7.6 / 信頼性 7.0 / 鮮度 5.5)

### 17. InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery
- URL: https://arxiv.org/abs/2602.08990
- Source: cs.AI updates on arXiv.org
- Score: 5.83 (新規性 5.0 / 実務影響 6.2 / 信頼性 7.0 / 鮮度 5.5)

### 18. Amazon may launch a marketplace where media sites can sell their content to AI companies
- URL: https://techcrunch.com/2026/02/10/amazon-may-launch-a-marketplace-where-media-sites-can-sell-their-content-to-ai-companies/
- Source: TechCrunch
- Score: 5.75 (新規性 5.8 / 実務影響 3.4 / 信頼性 6.0 / 鮮度 10.0)

### 19. Here’s how Rivian changed the rear door manual release on the R2
- URL: https://techcrunch.com/2026/02/10/heres-how-rivian-changed-the-rear-door-manual-release-on-the-r2/
- Source: TechCrunch
- Score: 5.75 (新規性 5.8 / 実務影響 3.4 / 信頼性 6.0 / 鮮度 10.0)

### 20. Drug Release Modeling using Physics-Informed Neural Networks
- URL: https://arxiv.org/abs/2602.09963
- Source: cs.LG updates on arXiv.org
- Score: 5.73 (新規性 5.8 / 実務影響 4.9 / 信頼性 7.0 / 鮮度 5.5)

### 21. Ex-GitHub CEO launches a new developer platform for AI agents
- URL: https://entire.io/blog/hello-entire-world/
- Source: Hacker News: Newest
- Score: 5.64 (新規性 7.0 / 実務影響 4.8 / 信頼性 5.0 / 鮮度 5.0)

### 22. AFABench: A Generic Framework for Benchmarking Active Feature Acquisition
- URL: https://arxiv.org/abs/2508.14734
- Source: cs.LG updates on arXiv.org
- Score: 5.54 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 3.0)

### 23. LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models
- URL: https://arxiv.org/abs/2411.00918
- Source: cs.LG updates on arXiv.org
- Score: 5.54 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 3.0)

### 24. Quantifying the Generalization Gap: A New Benchmark for Out-of-Distribution Graph-Based Android Malware Classification
- URL: https://arxiv.org/abs/2508.06734
- Source: cs.LG updates on arXiv.org
- Score: 5.54 (新規性 6.6 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 3.0)

### 25. CostNav: A Navigation Benchmark for Real-World Economic-Cost Evaluation of Physical AI Agents
- URL: https://arxiv.org/abs/2511.20216
- Source: cs.AI updates on arXiv.org
- Score: 5.54 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 3.0)

### 26. ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems
- URL: https://arxiv.org/abs/2409.01392
- Source: cs.AI updates on arXiv.org
- Score: 5.54 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 3.0)

### 27. Differentiable Modeling for Low-Inertia Grids: Benchmarking PINNs, NODEs, and DP for Identification and Control of SMIB System
- URL: https://arxiv.org/abs/2602.09667
- Source: eess.SY updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 28. TaCo: A Benchmark for Lossless and Lossy Codecs of Heterogeneous Tactile Data
- URL: https://arxiv.org/abs/2602.09893
- Source: cs.RO updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 29. Benchmarking the Energy Savings with Speculative Decoding Strategies
- URL: https://arxiv.org/abs/2602.09113
- Source: cs.LG updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 30. Statistical benchmarking of transformer models in low signal-to-noise time-series forecasting
- URL: https://arxiv.org/abs/2602.09869
- Source: cs.LG updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 31. AlgoVeri: An Aligned Benchmark for Verified Code Generation on Classical Algorithms
- URL: https://arxiv.org/abs/2602.09464
- Source: cs.CL updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 32. SupChain-Bench: Benchmarking Large Language Models for Real-World Supply Chain Management
- URL: https://arxiv.org/abs/2602.07342
- Source: cs.AI updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 33. GEBench: Benchmarking Image Generation Models as GUI Environments
- URL: https://arxiv.org/abs/2602.09007
- Source: cs.AI updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 34. VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing
- URL: https://arxiv.org/abs/2602.07045
- Source: cs.AI updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 35. WorldEdit: Towards Open-World Image Editing with a Knowledge-Informed Benchmark
- URL: https://arxiv.org/abs/2602.07095
- Source: cs.AI updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 36. aerial-autonomy-stack -- a Faster-than-real-time, Autopilot-agnostic, ROS2 Framework to Simulate and Deploy Perception-based Drones
- URL: https://arxiv.org/abs/2602.07264
- Source: cs.AI updates on arXiv.org
- Score: 5.50 (新規性 3.8 / 実務影響 6.5 / 信頼性 7.0 / 鮮度 5.5)

### 37. Fin-RATE: A Real-world Financial Analytics and Tracking Evaluation Benchmark for LLMs on SEC Filings
- URL: https://arxiv.org/abs/2602.07294
- Source: cs.AI updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 38. Gaussian Match-and-Copy: A Minimalist Benchmark for Studying Transformer Induction
- URL: https://arxiv.org/abs/2602.07562
- Source: cs.AI updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 39. MIND: Benchmarking Memory Consistency and Action Control in World Models
- URL: https://arxiv.org/abs/2602.08025
- Source: cs.AI updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 40. SWE Context Bench: A Benchmark for Context Learning in Coding
- URL: https://arxiv.org/abs/2602.08316
- Source: cs.AI updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

## Longlist (Top 30)
| Rank | Score | Title | Source | URL |
| :--- | ---: | :--- | :--- | :--- |
| 1 | 6.34 | LingxiDiagBench: A Multi-Agent Framework for Benchmarking LLMs in Chinese Psychiatric Consultation and Diagnosis | cs.CL updates on arXiv.org | [link](https://arxiv.org/abs/2602.09379) |
| 2 | 6.28 | MacrOData: New Benchmarks of Thousands of Datasets for Tabular Outlier Detection | cs.LG updates on arXiv.org | [link](https://arxiv.org/abs/2602.09329) |
| 3 | 6.02 | How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.07814) |
| 4 | 5.96 | MAPS: A Multilingual Benchmark for Agent Performance and Security | cs.CL updates on arXiv.org | [link](https://arxiv.org/abs/2505.15935) |
| 5 | 5.96 | Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.07434) |
| 6 | 5.92 | UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking | cs.RO updates on arXiv.org | [link](https://arxiv.org/abs/2602.10093) |
| 7 | 5.92 | ContextBench: A Benchmark for Context Retrieval in Coding Agents | cs.LG updates on arXiv.org | [link](https://arxiv.org/abs/2602.05892) |
| 8 | 5.92 | SWE-AGI: Benchmarking Specification-Driven Software Construction with MoonBit in the Era of Autonomous Agents | cs.CL updates on arXiv.org | [link](https://arxiv.org/abs/2602.09447) |
| 9 | 5.92 | LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.07962) |
| 10 | 5.92 | From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.08412) |
| 11 | 5.92 | CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.08023) |
| 12 | 5.92 | OmniReview: A Large-scale Benchmark and LLM-enhanced Framework for Realistic Reviewer Recommendation | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.08896) |
| 13 | 5.92 | ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.01655) |
| 14 | 5.90 | Reasoning With a Star: A Heliophysics Dataset and Benchmark for Agentic Scientific Reasoning | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2511.20694) |
| 15 | 5.86 | PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.07044) |
| 16 | 5.83 | NAAMSE: Framework for Evolutionary Security Evaluation of Agents | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.07391) |
| 17 | 5.83 | InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.08990) |
| 18 | 5.75 | Amazon may launch a marketplace where media sites can sell their content to AI companies | TechCrunch | [link](https://techcrunch.com/2026/02/10/amazon-may-launch-a-marketplace-where-media-sites-can-sell-their-content-to-ai-companies/) |
| 19 | 5.75 | Here’s how Rivian changed the rear door manual release on the R2 | TechCrunch | [link](https://techcrunch.com/2026/02/10/heres-how-rivian-changed-the-rear-door-manual-release-on-the-r2/) |
| 20 | 5.73 | Drug Release Modeling using Physics-Informed Neural Networks | cs.LG updates on arXiv.org | [link](https://arxiv.org/abs/2602.09963) |
| 21 | 5.64 | Ex-GitHub CEO launches a new developer platform for AI agents | Hacker News: Newest | [link](https://entire.io/blog/hello-entire-world/) |
| 22 | 5.54 | AFABench: A Generic Framework for Benchmarking Active Feature Acquisition | cs.LG updates on arXiv.org | [link](https://arxiv.org/abs/2508.14734) |
| 23 | 5.54 | LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models | cs.LG updates on arXiv.org | [link](https://arxiv.org/abs/2411.00918) |
| 24 | 5.54 | Quantifying the Generalization Gap: A New Benchmark for Out-of-Distribution Graph-Based Android Malware Classification | cs.LG updates on arXiv.org | [link](https://arxiv.org/abs/2508.06734) |
| 25 | 5.54 | CostNav: A Navigation Benchmark for Real-World Economic-Cost Evaluation of Physical AI Agents | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2511.20216) |
| 26 | 5.54 | ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2409.01392) |
| 27 | 5.50 | Differentiable Modeling for Low-Inertia Grids: Benchmarking PINNs, NODEs, and DP for Identification and Control of SMIB System | eess.SY updates on arXiv.org | [link](https://arxiv.org/abs/2602.09667) |
| 28 | 5.50 | TaCo: A Benchmark for Lossless and Lossy Codecs of Heterogeneous Tactile Data | cs.RO updates on arXiv.org | [link](https://arxiv.org/abs/2602.09893) |
| 29 | 5.50 | Benchmarking the Energy Savings with Speculative Decoding Strategies | cs.LG updates on arXiv.org | [link](https://arxiv.org/abs/2602.09113) |
| 30 | 5.50 | Statistical benchmarking of transformer models in low signal-to-noise time-series forecasting | cs.LG updates on arXiv.org | [link](https://arxiv.org/abs/2602.09869) |
