# 2026-02-10 Ranked News Candidates

- Source inbox: `2026-02-10_RSS_Links.md`
- Total parsed items: 2001
- Deduplicated items: 2001
- Selected items (top 30, min score 5.2): 30

## Scoring rubric
- Total = 0.35*技術新規性 + 0.30*実務影響 + 0.20*信頼性 + 0.15*鮮度
- Each axis is scored on a 0-10 scale

## Top Candidates

### 1. How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study
- URL: https://arxiv.org/abs/2602.07814
- Source: cs.AI updates on arXiv.org
- Score: 6.02 (新規性 6.9 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 2. Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots
- URL: https://arxiv.org/abs/2602.07434
- Source: cs.RO updates on arXiv.org
- Score: 5.96 (新規性 3.8 / 実務影響 8.0 / 信頼性 7.0 / 鮮度 5.5)

### 3. Benchmarking Autonomous Vehicles: A Driver Foundation Model Framework
- URL: https://arxiv.org/abs/2602.08298
- Source: cs.RO updates on arXiv.org
- Score: 5.92 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 5.5)

### 4. WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models
- URL: https://arxiv.org/abs/2602.08971
- Source: cs.RO updates on arXiv.org
- Score: 5.92 (新規性 6.6 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 5. UReason: Benchmarking the Reasoning Paradox in Unified Multimodal Models
- URL: https://arxiv.org/abs/2602.08336
- Source: cs.CL updates on arXiv.org
- Score: 5.92 (新規性 6.6 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 6. LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth
- URL: https://arxiv.org/abs/2602.07962
- Source: cs.AI updates on arXiv.org
- Score: 5.92 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 5.5)

### 7. From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent
- URL: https://arxiv.org/abs/2602.08412
- Source: cs.AI updates on arXiv.org
- Score: 5.92 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 5.5)

### 8. CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment
- URL: https://arxiv.org/abs/2602.08023
- Source: cs.AI updates on arXiv.org
- Score: 5.92 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 5.5)

### 9. OmniReview: A Large-scale Benchmark and LLM-enhanced Framework for Realistic Reviewer Recommendation
- URL: https://arxiv.org/abs/2602.08896
- Source: cs.AI updates on arXiv.org
- Score: 5.92 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 5.5)

### 10. ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development
- URL: https://arxiv.org/abs/2602.01655
- Source: cs.AI updates on arXiv.org
- Score: 5.92 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 5.5)

### 11. Reasoning With a Star: A Heliophysics Dataset and Benchmark for Agentic Scientific Reasoning
- URL: https://arxiv.org/abs/2511.20694
- Source: cs.LG updates on arXiv.org
- Score: 5.90 (新規性 5.4 / 実務影響 7.2 / 信頼性 7.0 / 鮮度 3.0)

### 12. Encoding Matters: Benchmarking Binary and D-ary Representations for Quantum Combinatorial Optimization
- URL: https://arxiv.org/abs/2602.07357
- Source: quant-ph updates on arXiv.org
- Score: 5.88 (新規性 5.4 / 実務影響 5.9 / 信頼性 7.0 / 鮮度 5.5)

### 13. Benchmarking the plasmon-pole and multipole approximations in the Yambo Code using the GW100 dataset
- URL: https://arxiv.org/abs/2602.07471
- Source: physics.chem-ph updates on arXiv.org
- Score: 5.86 (新規性 5.4 / 実務影響 5.8 / 信頼性 7.0 / 鮮度 5.5)

### 14. NLP for Local Governance Meeting Records: A Focus Article on Tasks, Datasets, Metrics and Benchmark
- URL: https://arxiv.org/abs/2602.08162
- Source: cs.CL updates on arXiv.org
- Score: 5.86 (新規性 5.4 / 実務影響 5.8 / 信頼性 7.0 / 鮮度 5.5)

### 15. ViGoEmotions: A Benchmark Dataset For Fine-grained Emotion Detection on Vietnamese Texts
- URL: https://arxiv.org/abs/2602.08371
- Source: cs.CL updates on arXiv.org
- Score: 5.86 (新規性 5.4 / 実務影響 5.8 / 信頼性 7.0 / 鮮度 5.5)

### 16. PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging
- URL: https://arxiv.org/abs/2602.07044
- Source: cs.AI updates on arXiv.org
- Score: 5.86 (新規性 5.4 / 実務影響 5.8 / 信頼性 7.0 / 鮮度 5.5)

### 17. NAAMSE: Framework for Evolutionary Security Evaluation of Agents
- URL: https://arxiv.org/abs/2602.07391
- Source: cs.AI updates on arXiv.org
- Score: 5.83 (新規性 3.8 / 実務影響 7.6 / 信頼性 7.0 / 鮮度 5.5)

### 18. InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery
- URL: https://arxiv.org/abs/2602.08990
- Source: cs.AI updates on arXiv.org
- Score: 5.83 (新規性 5.0 / 実務影響 6.2 / 信頼性 7.0 / 鮮度 5.5)

### 19. YouTube TV introduces cheaper bundles, including a $65/month sports package
- URL: https://techcrunch.com/2026/02/09/youtube-tv-introduces-cheaper-bundles-including-a-65-month-sports-package/
- Source: TechCrunch
- Score: 5.68 (新規性 5.6 / 実務影響 3.4 / 信頼性 6.0 / 鮮度 10.0)

### 20. The first signs of burnout are coming from the people who embrace AI the most
- URL: https://techcrunch.com/2026/02/09/the-first-signs-of-burnout-are-coming-from-the-people-who-embrace-ai-the-most/
- Source: TechCrunch
- Score: 5.64 (新規性 5.5 / 実務影響 3.4 / 信頼性 6.0 / 鮮度 10.0)

### 21. CostNav: A Navigation Benchmark for Real-World Economic-Cost Evaluation of Physical AI Agents
- URL: https://arxiv.org/abs/2511.20216
- Source: cs.RO updates on arXiv.org
- Score: 5.54 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 3.0)

### 22. ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems
- URL: https://arxiv.org/abs/2409.01392
- Source: cs.CL updates on arXiv.org
- Score: 5.54 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 3.0)

### 23. Towards Reliable Benchmarking: A Contamination Free, Controllable Evaluation Framework for Multi-step LLM Function Calling
- URL: https://arxiv.org/abs/2509.26553
- Source: cs.CL updates on arXiv.org
- Score: 5.54 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 3.0)

### 24. VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents
- URL: https://arxiv.org/abs/2510.11098
- Source: cs.CL updates on arXiv.org
- Score: 5.54 (新規性 5.4 / 実務影響 6.0 / 信頼性 7.0 / 鮮度 3.0)

### 25. Gaussian Match-and-Copy: A Minimalist Benchmark for Studying Transformer Induction
- URL: https://arxiv.org/abs/2602.07562
- Source: stat.ML updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 26. Beyond Arrow: From Impossibility to Possibilities in Multi-Criteria Benchmarking
- URL: https://arxiv.org/abs/2602.07593
- Source: stat.ML updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 27. aerial-autonomy-stack -- a Faster-than-real-time, Autopilot-agnostic, ROS2 Framework to Simulate and Deploy Perception-based Drones
- URL: https://arxiv.org/abs/2602.07264
- Source: cs.RO updates on arXiv.org
- Score: 5.50 (新規性 3.8 / 実務影響 6.5 / 信頼性 7.0 / 鮮度 5.5)

### 28. BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models
- URL: https://arxiv.org/abs/2602.08392
- Source: cs.RO updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 29. Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering
- URL: https://arxiv.org/abs/2602.08519
- Source: cs.LG updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

### 30. ViHERMES: A Graph-Grounded Multihop Question Answering Benchmark and System for Vietnamese Healthcare Regulations
- URL: https://arxiv.org/abs/2602.07361
- Source: cs.CL updates on arXiv.org
- Score: 5.50 (新規性 5.4 / 実務影響 4.6 / 信頼性 7.0 / 鮮度 5.5)

## Longlist (Top 30)
| Rank | Score | Title | Source | URL |
| :--- | ---: | :--- | :--- | :--- |
| 1 | 6.02 | How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.07814) |
| 2 | 5.96 | Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots | cs.RO updates on arXiv.org | [link](https://arxiv.org/abs/2602.07434) |
| 3 | 5.92 | Benchmarking Autonomous Vehicles: A Driver Foundation Model Framework | cs.RO updates on arXiv.org | [link](https://arxiv.org/abs/2602.08298) |
| 4 | 5.92 | WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models | cs.RO updates on arXiv.org | [link](https://arxiv.org/abs/2602.08971) |
| 5 | 5.92 | UReason: Benchmarking the Reasoning Paradox in Unified Multimodal Models | cs.CL updates on arXiv.org | [link](https://arxiv.org/abs/2602.08336) |
| 6 | 5.92 | LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.07962) |
| 7 | 5.92 | From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.08412) |
| 8 | 5.92 | CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.08023) |
| 9 | 5.92 | OmniReview: A Large-scale Benchmark and LLM-enhanced Framework for Realistic Reviewer Recommendation | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.08896) |
| 10 | 5.92 | ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.01655) |
| 11 | 5.90 | Reasoning With a Star: A Heliophysics Dataset and Benchmark for Agentic Scientific Reasoning | cs.LG updates on arXiv.org | [link](https://arxiv.org/abs/2511.20694) |
| 12 | 5.88 | Encoding Matters: Benchmarking Binary and D-ary Representations for Quantum Combinatorial Optimization | quant-ph updates on arXiv.org | [link](https://arxiv.org/abs/2602.07357) |
| 13 | 5.86 | Benchmarking the plasmon-pole and multipole approximations in the Yambo Code using the GW100 dataset | physics.chem-ph updates on arXiv.org | [link](https://arxiv.org/abs/2602.07471) |
| 14 | 5.86 | NLP for Local Governance Meeting Records: A Focus Article on Tasks, Datasets, Metrics and Benchmark | cs.CL updates on arXiv.org | [link](https://arxiv.org/abs/2602.08162) |
| 15 | 5.86 | ViGoEmotions: A Benchmark Dataset For Fine-grained Emotion Detection on Vietnamese Texts | cs.CL updates on arXiv.org | [link](https://arxiv.org/abs/2602.08371) |
| 16 | 5.86 | PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.07044) |
| 17 | 5.83 | NAAMSE: Framework for Evolutionary Security Evaluation of Agents | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.07391) |
| 18 | 5.83 | InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery | cs.AI updates on arXiv.org | [link](https://arxiv.org/abs/2602.08990) |
| 19 | 5.68 | YouTube TV introduces cheaper bundles, including a $65/month sports package | TechCrunch | [link](https://techcrunch.com/2026/02/09/youtube-tv-introduces-cheaper-bundles-including-a-65-month-sports-package/) |
| 20 | 5.64 | The first signs of burnout are coming from the people who embrace AI the most | TechCrunch | [link](https://techcrunch.com/2026/02/09/the-first-signs-of-burnout-are-coming-from-the-people-who-embrace-ai-the-most/) |
| 21 | 5.54 | CostNav: A Navigation Benchmark for Real-World Economic-Cost Evaluation of Physical AI Agents | cs.RO updates on arXiv.org | [link](https://arxiv.org/abs/2511.20216) |
| 22 | 5.54 | ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems | cs.CL updates on arXiv.org | [link](https://arxiv.org/abs/2409.01392) |
| 23 | 5.54 | Towards Reliable Benchmarking: A Contamination Free, Controllable Evaluation Framework for Multi-step LLM Function Calling | cs.CL updates on arXiv.org | [link](https://arxiv.org/abs/2509.26553) |
| 24 | 5.54 | VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents | cs.CL updates on arXiv.org | [link](https://arxiv.org/abs/2510.11098) |
| 25 | 5.50 | Gaussian Match-and-Copy: A Minimalist Benchmark for Studying Transformer Induction | stat.ML updates on arXiv.org | [link](https://arxiv.org/abs/2602.07562) |
| 26 | 5.50 | Beyond Arrow: From Impossibility to Possibilities in Multi-Criteria Benchmarking | stat.ML updates on arXiv.org | [link](https://arxiv.org/abs/2602.07593) |
| 27 | 5.50 | aerial-autonomy-stack -- a Faster-than-real-time, Autopilot-agnostic, ROS2 Framework to Simulate and Deploy Perception-based Drones | cs.RO updates on arXiv.org | [link](https://arxiv.org/abs/2602.07264) |
| 28 | 5.50 | BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models | cs.RO updates on arXiv.org | [link](https://arxiv.org/abs/2602.08392) |
| 29 | 5.50 | Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering | cs.LG updates on arXiv.org | [link](https://arxiv.org/abs/2602.08519) |
| 30 | 5.50 | ViHERMES: A Graph-Grounded Multihop Question Answering Benchmark and System for Vietnamese Healthcare Regulations | cs.CL updates on arXiv.org | [link](https://arxiv.org/abs/2602.07361) |
