# ContextBenchが暴いた盲点、コーディングAIは「どれだけ書けるか」より「どれだけ探せるか」
## 1,136タスクで見えたのは、生成性能の前にある文脈取得の壁
> コード生成の失敗は、推論不足より「必要情報の拾い方の失敗」で起きている可能性が高い。

## 結論（先に要点）
ContextBenchは、コーディングエージェントの文脈取得過程を独立評価し、既存ベンチマークでは見えにくかった中間失敗を可視化した。  
結果として、高度なエージェント構成を積んでも文脈取得は大幅に改善しにくく、前段工程の設計がボトルネックであることが示唆された。

対象論文: **ContextBench: A Benchmark for Context Retrieval in Coding Agents**  
公開日: 2026-02-05  
URL: https://arxiv.org/abs/2602.05892

---

## 先にひとことで言うと
- コーディングAIの改善はモデル変更だけでは足りず、文脈取得の品質管理をKPI化しないと伸びにくい。

---

## ここが意外だった
- よくある見方: 強いモデルと複雑なエージェント設計を使えば解決率は大きく上がる。
- この論文が示したこと: 文脈取得では、洗練されたスキャフォールド（実行枠組み）でも改善は限定的だった。
- それが重要な理由: 実務では解決率だけ見ても、どこで失敗したかが分からないと改善が回らない。

---

## この記事で分かること
- ContextBenchが既存評価に何を追加したか
- なぜ recall と precision のバランスが重要か
- 開発組織が導入すべき中間KPI

---

## 用語ミニ解説
- Context Retrieval: 問題解決に必要なコード文脈を取得する工程。
- Recall: 必要な情報を取りこぼさず取れた割合。
- Precision: 取った情報のうち本当に必要だった割合。

---

## 何がどう変わったのか（重要順）
1. 評価が最終成功率中心から、中間の文脈取得品質評価へ拡張した。  
2. エージェント改善の主戦場が、推論器より検索・参照設計へ移った。  
3. 「探索した文脈」と「実際に使った文脈」のギャップが重要指標として浮上した。  

---

## この研究は何をどう検証したのか
### データ設計
66リポジトリ・8言語から成る1,136件のissue解決タスクを構築。各タスクには人手アノテーションされたgold context（正解参照文脈）が付与された。

### タスク設計
issue解決の実行軌跡を追跡し、エージェントがどの文脈を探索し、どの文脈を利用したかを工程単位で評価する。

### 評価指標
- 文脈recall / precision
- 文脈取得効率
- 探索文脈と利用文脈の差分

---

## 主な結果（根拠つき）
1. **文脈取得は依然として難所である**  
根拠: 論文要旨で、複数のフロンティアLLMと5種のエージェントを評価しても、取得品質の改善が限定的と述べられている。

2. **多くのモデルは recall 偏重になりやすい**  
根拠: 要旨で「LLMs consistently favor recall over precision」と明記される。

3. **探索と利用の間に大きなギャップがある**  
根拠: 要旨で「substantial gaps exist between explored and utilized context」と報告される。

---

## 限界と注意点
- 分かっていること: 中間評価指標を持たないと、改善施策の当たり外れを判定しにくい。  
- まだ分からないこと: 超大規模モノレポや社内固有規約環境で同等の傾向が再現するかは追加検証が必要。  

---

## 実務チェックリスト
- [ ] 解決率に加えて context recall / precision を週次管理する
- [ ] エージェントの探索ログを保存し、利用文脈との差分を計測する
- [ ] 検索クエリ生成の失敗パターンを分類する
- [ ] モデル更新時は中間指標の回帰テストを行う
- [ ] 「拾いすぎ」「取りこぼし」を別々に改善する施策を持つ

---

## 背景と文脈（ボーナス）
ソフトウェア開発では、正しい関数を知っていてもファイル位置を誤ると実装は失敗する。ContextBenchは、この「知っていること」と「たどり着けること」の差をAI評価に持ち込んだ点が実務的に大きい。

---

## まとめ: この論文の応用例
1. **社内コーディングアシスタント評価**: 中間指標で失敗原因を可視化する。  
2. **AIレビュー自動化**: 関連差分の参照漏れを事前検知する。  
3. **障害解析支援**: ログ・コード・設定の探索品質を監査対象にする。  
4. **教育用途**: 「生成力」だけでなく「探索力」を学習目標に組み込む。  

---

## 参考文献
1. ContextBench: A Benchmark for Context Retrieval in Coding Agents. https://arxiv.org/abs/2602.05892
