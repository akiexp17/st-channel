# SAGEが示した逆説、Deep ResearchではBM25がLLM Retrieverを上回る局面がある
## 1,200クエリ検証で見えたのは「賢い検索器」より「問いの作り方」の問題だった
> 調査AIの精度を上げるには、モデルを強化する前に検索クエリ設計を見直す必要がある。

## 結論（先に要点）
SAGEは、Deep Researchエージェントの科学文献検索性能を体系的に評価し、既存エージェントでは推論負荷の高い検索が苦手であることを示した。  
特に重要なのは、LLMベースretrieverよりBM25が約30%高性能だったという結果で、ボトルネックがretriever性能単体よりクエリ設計にある可能性を示唆する点である。

対象論文: **SAGE: Benchmarking and Improving Retrieval for Deep Research Agents**  
公開日: 2026-02-05  
URL: https://arxiv.org/abs/2602.05975

---

## 先にひとことで言うと
- Deep Researchの品質は「どのretrieverを使うか」以上に「エージェントがどんな検索問いを作るか」で決まる。

---

## ここが意外だった
- よくある見方: LLMベースretrieverは伝統的IRより常に強い。
- この論文が示したこと: 現行エージェントではBM25がLLM retrieverを大きく上回る条件がある。
- それが重要な理由: 生成AI導入で見落とされがちな古典IRの有効性を再評価する必要がある。

---

## この記事で分かること
- SAGEベンチマークの設計と主要発見
- なぜBM25優位が起きたのかという実務的解釈
- 調査エージェント改善の優先順位

---

## 用語ミニ解説
- Deep Research Agent: 複数資料を探索し根拠付きで回答を作る調査AI。
- BM25: キーワード一致を重視する古典的検索アルゴリズム。
- Test-time Scaling: 学習後の実行時に追加計算や補助処理で性能を高める工夫。

---

## 何がどう変わったのか（重要順）
1. Deep Research検索の評価基盤（1,200クエリ/20万論文）が整備された。  
2. LLM retriever優位という前提が崩れ、クエリ生成品質が主要論点になった。  
3. 文書側をLLMで拡張する corpus-level test-time scaling の有効性が示された。  

---

## この研究は何をどう検証したのか
### データ設計
4科学分野・1,200クエリ・20万論文コーパスでSAGEを構築。短文質問とオープンエンド質問を含む調査条件を用意した。

### タスク設計
6つのDeep Researchエージェントを比較。DR Tuluを骨格に、BM25とLLM-based retrievers（ReasonIR, gte-Qwen2-7B-instruct）を差し替えて検索性能を比較した。

### 評価指標
- 検索精度（短文/オープンエンド）
- 推論負荷の高い検索での失敗率
- 改善手法導入後の性能差

---

## 主な結果（根拠つき）
1. **全システムが推論集約型検索で苦戦した**  
根拠: 要旨に「all systems struggle with reasoning-intensive retrieval」と明記。

2. **BM25がLLM retrieverを約30%上回った**  
根拠: 要旨に「BM25 significantly outperforms LLM-based retrievers by approximately 30%」と記載。

3. **文書拡張型のtest-time scalingで改善した**  
根拠: 要旨に、短文で8%、オープンエンドで2%の改善が報告される。

---

## 限界と注意点
- 分かっていること: 現状のDeep Researchエージェントは、retriever以前にクエリ設計改善が必要。  
- まだ分からないこと: 他分野・他言語・非科学ドメインで同様の差が維持されるかは未検証。  

---

## 実務チェックリスト
- [ ] LLM retriever導入前にBM25を必ずベースライン比較する
- [ ] サブクエリ生成の品質をログで監査する
- [ ] 検索失敗の原因を「retriever」と「query」に分離分析する
- [ ] 文書側メタデータ拡張の効果を小規模で検証する
- [ ] 調査タスクを短文と長文で分けて評価する

---

## 背景と文脈（ボーナス）
検索の世界では、新技術が出ても基礎手法が強い局面は繰り返し現れる。SAGEの結果は、生成AI時代でも「強い基礎＋適切な問い」が勝ち筋であることを示す。モデルを大きくする前に、検索設計を整える価値は高い。

---

## まとめ: この論文の応用例
1. **論文調査ワークフロー**: BM25を残しつつ、クエリ生成改善を優先する。  
2. **企業R&D探索**: 調査AI導入時に古典IRを捨てない設計を採用。  
3. **note記事制作**: 根拠探索プロセスを監査し、誤引用リスクを削減。  
4. **教育プログラム**: 検索器比較だけでなく、問い生成の質を評価項目に入れる。  

---

## 参考文献
1. SAGE: Benchmarking and Improving Retrieval for Deep Research Agents. https://arxiv.org/abs/2602.05975
